# Задание 1. Установить кластер k8s с 1 master node

## Установка k8s - control-node

***Отключить swap:***

```
sudo swapoff -a
```

```
sudo vim /etc/fstab
```

```
sudo mount -a
```

```
sudo rm /swapfile
```

***Включить IPv4 forwarding:***

```
sudo sysctl -w net.ipv4.ip_forward=1
```

```
sudo vim /etc/sysctl.conf
```

`+++ net.ipv4.ip_forward = 1`

```
sudo sysctl -p
```

```
cat /proc/sys/net/ipv4/ip_forward
```

***Установка среды для работы контейнеров***

```
sudo apt update && sudo apt install -y containerd
```

***Настройка containerd:***

- **Создаем директорию для конфига**

```
sudo mkdir -p /etc/containerd
```

- **Генерируем конфиг по умолчанию и сохраняем**
```
sudo containerd config default | sudo tee /etc/containerd/config.toml
```

- **Критически важный шаг:** *Меняем Cgroup Driver на "systemd", как рекомендует Kubernetes*.

```
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml
```

```
sudo systemctl restart containerd.service
```

```
sudo systemctl status containerd.service
```

***Установка зависимостей***

```
sudo apt-get update && sudo apt-get install -y apt-transport-https ca-certificates curl gpg
```

***Подключение репозитория kubernetes***

```
sudo mkdir -p -m 755 /etc/apt/keyrings 
```

```
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.33/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
```

```
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.33/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
```

***Установка основных компонентов***

```
sudo apt update && sudo apt install -y kubelet kubeadm kubectl
```

**Включите сервис kubelet:***  

```
sudo systemctl enable --now kubelet
```

***Инициализация master-ноды***:

```
sudo kubeadm init --apiserver-advertise-address=10.129.0.21 --pod-network-cidr 10.244.0.0/16 --apiserver-cert-extra-sans=158.160.77.70 --control-plane-endpoint=cluster_ip_address
```

> [!NOTE] Описание
> 10.129.0.21 — адрес, который слушает apiserver
> 10.244.0.0/16 — сеть для подов
> 158.160.77.70 — внешний адрес для подключения через kubectl
> cluster_ip_address — кластерный IP-адрес (адрес LoadBalancer) для HA control plane
--control-plane-endpoint string
***Вывод***

![[Снимок экрана от 2025-12-25 12-04-49.png]]
**После успешного выполнения команда покажет токен для подключения** **worker****-****нод**

![[Снимок экрана от 2025-12-25 12-14-30.png]]

***Подключаем kubectl к k8s***

```
kubectl version --client
```

```
mkdir -p $HOME/.kube
```

```
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
```

```
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

> [!important] Важно
> В конфигурационном файле изменить ip адрес с внешнего на внутренний

```
source <(kubectl completion bash)
```

```
echo "source <(kubectl completion bash)" >> ~/.bashrc
```

```
source .bashrc
```

***Проверяем через kubectl***

```
kubectl get nodes
```

***Установить сетевой плагин calico***

```
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.31.3/manifests/operator-crds.yaml
```

```
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.31.3/manifests/tigera-operator.yaml
```

```
curl -O https://raw.githubusercontent.com/projectcalico/calico/v3.31.3/manifests/custom-resources-bpf.yaml
```


> [!NOTE] Замечание
> Изменить ip адрес в скачанном файле custom-resources-bpf.yaml на сеть подов 10.244.0.0/16 с которой мы инициализировали наш кластер

![[Снимок экрана от 2025-12-25 17-14-28.png]]

```
kubectl create -f custom-resources-bpf.yaml
```

***отслеживаем процесс установки:***
```
watch kubectl get tigerastatus
```

![[Снимок экрана от 2025-12-25 17-15-43.png]]

***Проверяем статус подов чтобы они были запущены***

```
kubectl get pods -A
```

![[Снимок экрана от 2025-12-25 17-17-00.png]]

***Проверяем статус нашего кластера***

```
kubectl get nodes
```

![[Снимок экрана от 2025-12-25 17-18-38.png]]


> [!NOTE] Заключение
> Если наш cluster STATUS Ready можно переходить к установке на рабочие ноды

***Команда для создания токена для подключения к copntol-node***

```
kubeadm token create --print-join-command
```

## Установка k8s - worker-node

```
sudo swapoff -a
```

```
sudo vim /etc/fstab
```

```
sudo mount -a
```

```
sudo rm /swapfile
```

```
sudo sysctl -w net.ipv4.ip_forward=1
```

```
sudo vim /etc/sysctl.conf
```

`+++ net.ipv4.ip_forward = 1`

```
sudo sysctl -p
```

```
cat /proc/sys/net/ipv4/ip_forward
```

```
sudo apt update && sudo apt install -y containerd
```

```
sudo mkdir -p /etc/containerd
```

```
sudo containerd config default | sudo tee /etc/containerd/config.toml
```

```
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml
```

```
sudo systemctl restart containerd.service
```

```

sudo systemctl status containerd.service
```

```
sudo apt-get update && sudo apt-get install -y apt-transport-https ca-certificates curl gpg
```

```
sudo mkdir -p -m 755 /etc/apt/keyrings 
```

```
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.33/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
```

```
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.33/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list
```

```
sudo apt update && sudo apt install -y kubelet kubeadm kubectl
```

```
sudo systemctl enable --now kubelet
```

```
sudo kubeadm join 158.160.86.32:6443 --token zea04u.fczk9bwo4094r8vi --discovery-token-ca-cert-hash sha256:144656bcad096108b5ba017b91f54ac07d4feee627e858e0da3154b418395b3f 
```

## Развернутый кластер:

![[Снимок экрана от 2025-12-25 17-54-07.png]]

![[Снимок экрана от 2025-12-25 17-54-19.png]]

## Дополнительные задания (со звёздочкой)

### Задание 2*. Установить HA кластер

#### Следующий этап после установки kubeadm, kubelet ...

##### 1 control-plane

```
sudo kubeadm init --apiserver-advertise-address=10.129.0.11 --pod-network-cidr 10.244.0.0/16 --apiserver-cert-extra-sans=158.160.73.87 --control-plane-endpoint=130.193.57.207 --upload-certs
```

##### 2 control-plane

```
sudo kubeadm join 130.193.57.207:6443 --token cyem7y.x5gfxybu8ldv1h10 --discovery-token-ca-cert-hash sha256:999ee7f7b2e907e4a8029ce5b105868fce4fc96bcd8941a8814dcb7efd977cf1 --control-plane --certificate-key  5726ee5cce79f6a143376c42d3e8547ed669ec8f79d8fe4dc9d475d40d452284
```

#### 3 control-plane

```
sudo kubeadm join 130.193.57.207:6443 --token cyem7y.x5gfxybu8ldv1h10 --discovery-token-ca-cert-hash sha256:999ee7f7b2e907e4a8029ce5b105868fce4fc96bcd8941a8814dcb7efd977cf1 --control-plane --certificate-key  5726ee5cce79f6a143376c42d3e8547ed669ec8f79d8fe4dc9d475d40d452284
```

![[Снимок экрана от 2025-12-26 15-11-25.png]]
##### На всех master нодах

***Подключился к кластеру***

```
kubeadm join 130.193.57.207:6443 --token 5cu72l.svjz9epkqop0il1c \
	--discovery-token-ca-cert-hash sha256:4a36bf1abedc889e6c66490169751918eea439d7674f97f60df6385ec509664e 
```

```
mkdir -p $HOME/.kube
```

```
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
```

```
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

> [!important] Важно
> В конфигурационном файле изменить ip адрес с внешнего на внутренний

```
source <(kubectl completion bash)
```

```
echo "source <(kubectl completion bash)" >> ~/.bashrc
```

```
source .bashrc
```

***Проверяем через kubectl***

```
kubectl get nodes
```

***Установить сетевой плагин calico***

```
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.31.3/manifests/operator-crds.yaml
```

```
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.31.3/manifests/tigera-operator.yaml
```

```
curl -O https://raw.githubusercontent.com/projectcalico/calico/v3.31.3/manifests/custom-resources-bpf.yaml
```


> [!NOTE] Замечание
> Изменить ip адрес в скачанном файле custom-resources-bpf.yaml на сеть подов 10.244.0.0/16 с которой мы инициализировали наш кластер

![[Снимок экрана от 2025-12-25 17-14-28.png]]

```
kubectl create -f custom-resources-bpf.yaml
```

***отслеживаем процесс установки:***
```
watch kubectl get tigerastatus
```

![[Снимок экрана от 2025-12-25 17-15-43.png]]

***Проверяем статус подов чтобы они были запущены***

```
kubectl get pods -A
```

![[Снимок экрана от 2025-12-25 17-17-00.png]]

***Проверяем статус нашего кластера***

```
kubectl get nodes
```

![[Снимок экрана от 2025-12-26 15-27-34.png]]

